# ============================================
# DOCUMENT Q&A SYSTEM - ENVIRONMENT CONFIGURATION
# ============================================
# Copy this file to .env and fill in your actual values
# Never commit .env to version control!

# ============================================
# LANGSMITH CONFIGURATION (Optional)
# ============================================
# LangSmith provides tracing and monitoring for LLM applications
# Sign up at: https://smith.langchain.com/
# Get API key from: https://smith.langchain.com/settings

LANGCHAIN_TRACING_V2=false
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=your_langsmith_api_key_here
LANGCHAIN_PROJECT=document-qna-app

# ============================================
# OLLAMA CONFIGURATION (Local LLM)
# ============================================
# Ollama provides local LLM models
# Install from: https://ollama.com/download

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# ============================================
# APPLICATION SETTINGS
# ============================================

APP_NAME=Document Q&A System
MAX_PROJECTS_PER_USER=10
MAX_DOCUMENTS_PER_PROJECT=50
MAX_FILE_SIZE_MB=50

# Text chunking for vector storage
CHUNK_SIZE=700
CHUNK_OVERLAP=250

# ============================================
# DATABASE CONFIGURATION
# ============================================

DATABASE_PATH=data/app.db

# ============================================
# VECTOR STORE CONFIGURATION
# ============================================

CHROMA_PERSIST_DIRECTORY=data/projects

# ============================================
# SEARCH CONFIGURATION
# ============================================

TOP_K_RESULTS=10
SEARCH_TYPE=similarity

# ============================================
# LLM PARAMETERS
# ============================================

TEMPERATURE=0.3
MAX_TOKENS=2000

# ============================================
# CLOUD PROVIDER CONFIGURATION
# ============================================
# Choose between 'local' (Ollama) or 'gemini' (Google Cloud)
# Local: Free, private, slower
# Gemini: Fast (5-10x), free tier available

LLM_PROVIDER=local
VISION_PROVIDER=local

# ============================================
# GOOGLE GEMINI CONFIGURATION (Optional)
# ============================================
# Google Gemini provides fast cloud-based AI
# Free tier: 60 requests/minute, 1,500/day
# Get API key from: https://makersuite.google.com/app/apikey

GOOGLE_API_KEY=your_google_api_key_here
GEMINI_MODEL=gemini-2.5-flash
GEMINI_VISION_MODEL=gemini-2.5-flash

# ============================================
# HUGGINGFACE CONFIGURATION (Optional)
# ============================================
# For faster embeddings or additional models
# Get token from: https://huggingface.co/settings/tokens

# HUGGINGFACE_API_KEY=your_hf_token_here
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================
# NOTES
# ============================================
# 1. This is a template file - copy to .env and fill in real values
# 2. Never commit .env file to git (it's in .gitignore)
# 3. Start with local models (Ollama) for privacy
# 4. Add Gemini API key for 5-10x faster responses
# 5. LangSmith is optional for debugging/monitoring
